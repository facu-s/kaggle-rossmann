{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 20:06:56.730869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-26 20:07:18.245760: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-26 20:07:18.300648: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-26 20:07:18.422775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-26 20:07:18.747689: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-26 20:07:18.747735: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=1-NYPQw5THU&feature=youtu.be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('train_normalized_data.fth')\n",
    "df_test = pd.read_feather('test_normalized_data.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen', 'Promo2Weeks', \n",
    "            'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State', \n",
    "            'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
    "cat_vars = ['Store','Year','Month','StateHoliday','CompetitionMonthsOpen',\n",
    "    'Promo2Weeks','StoreType','Assortment','PromoInterval',\n",
    "    'CompetitionOpenSinceYear','State',\n",
    "    'Week','Events','Promo_fw','Promo_bw','StateHoliday_bool_fw', 'StateHoliday_bool_bw','SchoolHoliday_fw','SchoolHoliday_bw']#'DayOfWeek'\n",
    "\n",
    "# cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'State']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contin_vars = ['CompetitionDistance', \n",
    "   'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n",
    "contin_vars = ['CompetitionDistance', \n",
    "   'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Precipitationmm',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', \n",
    " 'CloudCover', 'trend',\n",
    "   'AfterStateHoliday_bool', 'BeforeStateHoliday_bool', 'Promo', 'SchoolHoliday', 'StateHoliday_bool']\n",
    "cat_vars = ['Store', 'DayOfWeek', 'Promo_bw', 'Month', 'Year', 'State', 'Assortment', 'StoreType', 'Events', 'StateHoliday', 'Promo2SinceYear']\n",
    "contin_vars = [ 'Max_TemperatureC', 'Precipitationmm', 'Promo', 'SchoolHoliday', 'trend', 'Mean_TemperatureC','CompetitionDistance']\n",
    "# contin_vars = []\n",
    "\n",
    "# cat_vars = ['Store',\n",
    "#   'DayOfWeek',\n",
    "#   'Year',\n",
    "#   'Month',\n",
    "#   'Day',\n",
    "#   'StateHoliday',\n",
    "#   'CompetitionMonthsOpen',\n",
    "#   'Promo2Weeks',\n",
    "#   'StoreType',\n",
    "#   'Assortment',\n",
    "#   'PromoInterval',\n",
    "#   'CompetitionOpenSinceYear',\n",
    "#   'Promo2SinceYear',\n",
    "#   'State',\n",
    "#   'Week',\n",
    "#   'Events',\n",
    "#   'Promo_fw',\n",
    "#   'Promo_bw',\n",
    "#   'SchoolHoliday_fw',\n",
    "#   'SchoolHoliday_bw']\n",
    "# contin_vars = ['CompetitionDistance', \n",
    "#    'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Precipitationmm',\n",
    "#    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "#    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "#    'AfterStateHoliday_bool', 'BeforeStateHoliday_bool', 'Promo', 'SchoolHoliday', 'StateHoliday_bool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_columns = ['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad en val: 30188, porcentaje: 0.9642465458145908\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df.Date < datetime.datetime(2015, 7, 1)]  \n",
    "df_val = df[df.Date >= datetime.datetime(2015, 7, 1)]\n",
    "print(f'Cantidad en val: {len(df_val)}, porcentaje: {len(df_train)/(len(df_train) + len(df_val))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[cat_vars + contin_vars]\n",
    "X_valid = df_val[cat_vars + contin_vars]\n",
    "X_test = df_test[cat_vars + contin_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((814150, 18), (30188, 18))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/facu/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "log_output = True\n",
    "    \n",
    "if log_output:\n",
    "    # Escala logaritmica\n",
    "    max_log_y = np.max(np.log(df[y_out_columns])).values\n",
    "    y_train = np.log(df_train[y_out_columns].values)/max_log_y\n",
    "    y_val = np.log(df_val[y_out_columns].values)/max_log_y\n",
    "else:\n",
    "    # Normalizaci√≥n\n",
    "    y_mean = df_train[y_out_columns].mean().values\n",
    "    y_std = df_train[y_out_columns].std().values\n",
    "    y_train = (df_train[y_out_columns].values - y_mean)/y_std\n",
    "    y_val = (df_val[y_out_columns].values - y_mean)/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoricals_processing = 'no_categoricals'\n",
    "# categoricals_processing = 'use_onehotencoding'\n",
    "categoricals_processing = 'use_embeddings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# n=len(df.Sales)\n",
    "\n",
    "# train_ratio=0.9\n",
    "# train_size = int(n*train_ratio)\n",
    "# # contins_trn, contins_val = df[contin_vars].loc[:train_size-1], df[contin_vars].loc[train_size:]\n",
    "# # cats_trn, cats_val       = df[cat_var_dict.keys()].loc[:train_size-1], df[cat_var_dict.keys()].loc[train_size:]\n",
    "\n",
    "# X_train,X_valid                  = df[cat_vars+contin_vars].loc[:train_size-1], df[cat_vars+contin_vars].loc[train_size:]\n",
    "\n",
    "# max_y_log                        = (np.log(df.Sales)).values.max()\n",
    "# y_train, y_valid                 = np.log(df.Sales.loc[:train_size-1])/max_y_log, np.log(df.Sales.loc[train_size:])/max_y_log\n",
    "\n",
    "# print(\"X_train[\",type(X_train),\"]: \",X_train.shape,\"      y_train[\",type(y_train),\"]: \",y_train.shape)\n",
    "# print(\"X_valid[\",type(X_train),\"]: \",X_valid.shape,\"      y_valid[\",type(y_valid),\"]: \",y_valid.shape)\n",
    "\n",
    "# tscv = TimeSeriesSplit(n_splits=2, test_size=(n - train_size))\n",
    "# display('test_size: ',(n - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 20:07:26.332234: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-26 20:07:26.333370: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 20:07:27.679349: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-02-26 20:07:27.796111: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25443/25443 [==============================] - 25s 916us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 20:07:52.411455: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 283324200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 [==============================] - 1s 905us/step\n",
      "1284/1284 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "if categoricals_processing == 'use_embeddings':\n",
    "    embeddings_model = load_model('embeddings_model.hdf5')\n",
    "    # display(embeddings_model.summary())\n",
    "    X_train = embeddings_model.predict(np.hsplit(X_train, X_train.shape[1]), verbose=1)\n",
    "    X_valid = embeddings_model.predict(np.hsplit(X_valid, X_valid.shape[1]), verbose=1)\n",
    "    X_test = embeddings_model.predict(np.hsplit(X_test, X_test.shape[1]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if categoricals_processing == 'use_onehotencoding':\n",
    "    # Use One Hot Encoding\n",
    "    categorical_feature_mask = [col in cat_vars for col in X_train.columns]\n",
    "    ohe = OneHotEncoder(categorical_features = categorical_feature_mask)\n",
    "    ohe.fit(X_train)\n",
    "    X_train = ohe.transform(X_train)\n",
    "    X_valid = ohe.transform(X_valid)\n",
    "    X_test = ohe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814150, 87)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30188, 87)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41088, 87)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=len(df.Sales)\n",
    "\n",
    "# train_ratio=0.9\n",
    "# train_size = int(n*train_ratio)\n",
    "# tscv = TimeSeriesSplit(n_splits=2, test_size=(n - train_size))\n",
    "# for train_index, test_index in tscv.split(df[cat_vars+contin_vars]):\n",
    "#     print(\"-----------\")\n",
    "#     print(train_index)\n",
    "#     print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import get_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(((y_true - y_pred)/y_true)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     'n_estimators'      :[3500,4500],\n",
    "#     'eta'               :[0.1,0.2], \n",
    "#     'objective'         :['reg:squarederror'],\n",
    "#     'n_jobs'            :[4],\n",
    "#     'max_depth'         :[3,5],\n",
    "#     'subsample'         :[0.8],#[0.2,0.8],\n",
    "#     'colsample_bytree'  :[0.8],#[0.2,0.8],\n",
    "#     'alpha'             :[0.2,0.3,0.5],\n",
    "#     'gamma'             :[0]\n",
    "#                     }\n",
    "# parameters   = { 'n_estimators'      : 4000,\n",
    "#                  'learning_rate'     : 0.2, \n",
    "#                  'max_depth'         : 13,\n",
    "#                  'max_leaves'        : 0, #DEFAULT: 0 (unlimited)\n",
    "#                  'gamma'             : 0.004,\n",
    "#                  'reg_alpha'         : 0.07,\n",
    "#                  #'reg_lambda'         : 0.2,\n",
    "#                }\n",
    "\n",
    "# grid_search_rmspe = make_scorer(rmspe,greater_is_better=False)\n",
    "# model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(model, parameters, n_jobs = 4, verbose=100, cv=2, scoring=grid_search_rmspe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(alpha=0.07, base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eta=0.2, eval_metric=None,\n",
       "             feature_types=None, gamma=0.005, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=13,\n",
       "             max_leaves=None, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=3000, n_jobs=-1,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(alpha=0.07, base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eta=0.2, eval_metric=None,\n",
       "             feature_types=None, gamma=0.005, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=13,\n",
       "             max_leaves=None, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=3000, n_jobs=-1,\n",
       "             num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(alpha=0.07, base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eta=0.2, eval_metric=None,\n",
       "             feature_types=None, gamma=0.005, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=13,\n",
       "             max_leaves=None, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=3000, n_jobs=-1,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators= 3000\n",
    "learning_rate=0.2\n",
    "model = XGBRegressor(n_estimators=n_estimators,\n",
    "                     eta=learning_rate, \n",
    "                     objective='reg:squarederror',\n",
    "                     n_jobs=-1,\n",
    "                     max_depth=13,\n",
    "                     min_child_weight = 1,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     alpha=0.07,\n",
    "                     gamma=0.005\n",
    "                    )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params={\"early_stopping_rounds\":100, \n",
    "            \"eval_metric\" : 'rmse', \n",
    "            # \"eval_set\" : [(X_valid, y_valid)],\n",
    "            \"eval_set\" : [(X_valid, y_val.reshape(-1))],\n",
    "            'verbose': 100,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/facu/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/home/facu/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.26287\n",
      "[100]\tvalidation_0-rmse:0.01257\n",
      "[200]\tvalidation_0-rmse:0.01240\n",
      "[300]\tvalidation_0-rmse:0.01233\n",
      "[400]\tvalidation_0-rmse:0.01228\n",
      "[500]\tvalidation_0-rmse:0.01228\n",
      "[600]\tvalidation_0-rmse:0.01224\n",
      "[700]\tvalidation_0-rmse:0.01222\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train.reshape(-1), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©trica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{\\hat{y}_i - y_i}{y_i}\\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X_valid, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_output:\n",
    "    y_pred_train = np.exp(model.predict(X_train)*max_log_y)\n",
    "    y_pred = np.exp(model.predict(X_valid)*max_log_y)\n",
    "    y_pred_test = np.exp(model.predict(X_test)*max_log_y)\n",
    "else:\n",
    "    y_pred_train = model.predict(X_train)*y_std + y_mean\n",
    "    y_pred = model.predict(X_val)*y_std + y_mean\n",
    "    y_pred_test = model.predict(X_test)*y_std + y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_RMSE = np.sqrt((((df_train['Sales'].values - y_pred_train)/df_train['Sales'].values)**2).sum()/len(y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n\n",
    "val_RMSE = np.sqrt((((df_val['Sales'].values - y_pred)/df_val['Sales'].values)**2).sum()/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886748132936197 0.09842086748103157 0.13625017955745045\n"
     ]
    }
   ],
   "source": [
    "print(score, train_RMSE, val_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_csv = pd.read_csv('dataset/rossmann/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_mean = {}\n",
    "for store, g_df in df.groupby('Store'):\n",
    "    stores_mean[store] = g_df[g_df['Sales'] > 0]['Sales'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Sales'] = df_test['Store'].apply(stores_mean.get)\n",
    "df_test.loc[df_test['Open'] == 0, 'Sales'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['Store', 'Sales']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['Open'] == 0][['Store', 'Sales']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv['Sales'] = df_test['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv.to_csv(f'submision_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumbit a la competici√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv = pd.read_csv('rossmann/sample_submission.csv')\n",
    "sample_csv['Sales'] = y_pred_test\n",
    "sample_csv.head()\n",
    "\n",
    "sample_csv.to_csv(f'submision_XGBoost.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
